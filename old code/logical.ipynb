{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.00024\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 100 | loss: 0.00024 -- iter: 2/2\n",
      "Testing NOT operator\n",
      "NOT 0: [[0.9849451]]\n",
      "NOT 1: [[0.01189029]]\n"
     ]
    }
   ],
   "source": [
    "# Logical NOT operator\n",
    "X = [[0.], [1.]]\n",
    "Y = [[1.], [0.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 1])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing NOT operator\")\n",
    "    print(\"NOT 0:\", m.predict([[0.]]))\n",
    "    print(\"NOT 1:\", m.predict([[1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.02596\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 100 | loss: 0.02596 -- iter: 4/4\n",
      "Testing OR operator\n",
      "0 or 0: [[0.03754679]]\n",
      "0 or 1: [[0.98312205]]\n",
      "1 or 0: [[0.97406423]]\n",
      "1 or 1: [[0.9999821]]\n"
     ]
    }
   ],
   "source": [
    "# Logical OR operator\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y = [[0.], [1.], [1.], [1.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing OR operator\")\n",
    "    print(\"0 or 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 or 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 or 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 or 1:\", m.predict([[1., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.00111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 100 | loss: 0.00111 -- iter: 4/4\n",
      "Testing AND operator\n",
      "0 and 0: [[2.586378e-05]]\n",
      "0 and 1: [[0.02738011]]\n",
      "1 and 0: [[0.02738024]]\n",
      "1 and 1: [[0.96839404]]\n"
     ]
    }
   ],
   "source": [
    "# Logical AND operator\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y = [[0.], [0.], [0.], [1.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=False)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing AND operator\")\n",
    "    print(\"0 and 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 and 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 and 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 and 1:\", m.predict([[1., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Going further: Graph combination with multiple optimizers\n",
    "# Create a XOR operator using product of NAND and OR operators\n",
    "# '''\n",
    "# Data\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y_nand = [[1.], [1.], [1.], [0.]]\n",
    "Y_or = [[0.], [1.], [1.], [1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.82293\u001b[0m\u001b[0m | time: 0.027s\n",
      "| SGD_0 | epoch: 400 | loss: 0.40930 -- iter: 4/4\n",
      "| SGD_1 | epoch: 400 | loss: 0.41363 -- iter: 4/4\n",
      "Testing XOR operator\n",
      "0 xor 0: [[0.00072242]]\n",
      "0 xor 1: [[0.9968404]]\n",
      "1 xor 0: [[0.9969938]]\n",
      "1 xor 1: [[0.00177113]]\n"
     ]
    }
   ],
   "source": [
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    # Building a network with 2 optimizers\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    # Nand operator definition\n",
    "    g_nand = tflearn.fully_connected(g, 32, activation='linear')\n",
    "    g_nand = tflearn.fully_connected(g_nand, 32, activation='linear')\n",
    "    g_nand = tflearn.fully_connected(g_nand, 1, activation='sigmoid')\n",
    "    g_nand = tflearn.regression(g_nand, optimizer='sgd',\n",
    "                                learning_rate=2.,\n",
    "                                loss='binary_crossentropy')\n",
    "    # Or operator definition\n",
    "    g_or = tflearn.fully_connected(g, 32, activation='linear')\n",
    "    g_or = tflearn.fully_connected(g_or, 32, activation='linear')\n",
    "    g_or = tflearn.fully_connected(g_or, 1, activation='sigmoid')\n",
    "    g_or = tflearn.regression(g_or, optimizer='sgd',\n",
    "                              learning_rate=2.,\n",
    "                              loss='binary_crossentropy')\n",
    "    # XOR merging Nand and Or operators\n",
    "    g_xor = tflearn.merge([g_nand, g_or], mode='elemwise_mul')\n",
    "\n",
    "    # Training\n",
    "    m = tflearn.DNN(g_xor)\n",
    "    m.fit(X, [Y_nand, Y_or], n_epoch=400, snapshot_epoch=False)\n",
    "\n",
    "    # Testing\n",
    "    print(\"Testing XOR operator\")\n",
    "    print(\"0 xor 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 xor 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 xor 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 xor 1:\", m.predict([[1., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 2DY7KI\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 4\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.093s\n",
      "| SGD | epoch: 001 | loss: 0.00000 -- iter: 4/4\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.22500\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 002 | loss: 0.22500 -- iter: 4/4\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.22240\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 003 | loss: 0.22240 -- iter: 4/4\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.21076\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 004 | loss: 0.21076 -- iter: 4/4\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.20252\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 005 | loss: 0.20252 -- iter: 4/4\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.19730\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 006 | loss: 0.19730 -- iter: 4/4\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.19399\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 007 | loss: 0.19399 -- iter: 4/4\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.19186\u001b[0m\u001b[0m | time: 0.036s\n",
      "| SGD | epoch: 008 | loss: 0.19186 -- iter: 4/4\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.19045\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 009 | loss: 0.19045 -- iter: 4/4\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.18949\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 010 | loss: 0.18949 -- iter: 4/4\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.18882\u001b[0m\u001b[0m | time: 0.028s\n",
      "| SGD | epoch: 011 | loss: 0.18882 -- iter: 4/4\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.18834\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 012 | loss: 0.18834 -- iter: 4/4\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.18798\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 013 | loss: 0.18798 -- iter: 4/4\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.18770\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 014 | loss: 0.18770 -- iter: 4/4\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.18748\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 015 | loss: 0.18748 -- iter: 4/4\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.18729\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 016 | loss: 0.18729 -- iter: 4/4\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.18713\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 017 | loss: 0.18713 -- iter: 4/4\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.18697\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 018 | loss: 0.18697 -- iter: 4/4\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.18682\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 019 | loss: 0.18682 -- iter: 4/4\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.18668\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 020 | loss: 0.18668 -- iter: 4/4\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.18652\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 021 | loss: 0.18652 -- iter: 4/4\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.18636\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 022 | loss: 0.18636 -- iter: 4/4\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.18617\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 023 | loss: 0.18617 -- iter: 4/4\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.18597\u001b[0m\u001b[0m | time: 0.042s\n",
      "| SGD | epoch: 024 | loss: 0.18597 -- iter: 4/4\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.18575\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 025 | loss: 0.18575 -- iter: 4/4\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.18548\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 026 | loss: 0.18548 -- iter: 4/4\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.18518\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 027 | loss: 0.18518 -- iter: 4/4\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.18483\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 028 | loss: 0.18483 -- iter: 4/4\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.18442\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 029 | loss: 0.18442 -- iter: 4/4\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.18392\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 030 | loss: 0.18392 -- iter: 4/4\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.18332\u001b[0m\u001b[0m | time: 0.019s\n",
      "| SGD | epoch: 031 | loss: 0.18332 -- iter: 4/4\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.18259\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 032 | loss: 0.18259 -- iter: 4/4\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.18169\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 033 | loss: 0.18169 -- iter: 4/4\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.18056\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 034 | loss: 0.18056 -- iter: 4/4\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.17912\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 035 | loss: 0.17912 -- iter: 4/4\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.17728\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 036 | loss: 0.17728 -- iter: 4/4\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.17488\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 037 | loss: 0.17488 -- iter: 4/4\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.17173\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 038 | loss: 0.17173 -- iter: 4/4\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.16758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 039 | loss: 0.16758 -- iter: 4/4\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.16223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 040 | loss: 0.16223 -- iter: 4/4\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.15557\u001b[0m\u001b[0m | time: 0.020s\n",
      "| SGD | epoch: 041 | loss: 0.15557 -- iter: 4/4\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.14769\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 042 | loss: 0.14769 -- iter: 4/4\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.13876\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 043 | loss: 0.13876 -- iter: 4/4\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.12907\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 044 | loss: 0.12907 -- iter: 4/4\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.11893\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 045 | loss: 0.11893 -- iter: 4/4\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.10866\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 046 | loss: 0.10866 -- iter: 4/4\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.09853\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 047 | loss: 0.09853 -- iter: 4/4\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.08879\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 048 | loss: 0.08879 -- iter: 4/4\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.07960\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 049 | loss: 0.07960 -- iter: 4/4\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.07109\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 050 | loss: 0.07109 -- iter: 4/4\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.06333\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 051 | loss: 0.06333 -- iter: 4/4\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.05632\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 052 | loss: 0.05632 -- iter: 4/4\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.05006\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 053 | loss: 0.05006 -- iter: 4/4\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.04449\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 054 | loss: 0.04449 -- iter: 4/4\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.03956\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 055 | loss: 0.03956 -- iter: 4/4\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.03521\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 056 | loss: 0.03521 -- iter: 4/4\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.03137\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 057 | loss: 0.03137 -- iter: 4/4\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.02799\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 058 | loss: 0.02799 -- iter: 4/4\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 059 | loss: 0.02501 -- iter: 4/4\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 060 | loss: 0.02239 -- iter: 4/4\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 061 | loss: 0.02008 -- iter: 4/4\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 062 | loss: 0.01804 -- iter: 4/4\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.01624\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 063 | loss: 0.01624 -- iter: 4/4\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.01466\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 064 | loss: 0.01466 -- iter: 4/4\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.01325\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 065 | loss: 0.01325 -- iter: 4/4\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.06858\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 066 | loss: 0.06858 -- iter: 4/4\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.08223\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 067 | loss: 0.08223 -- iter: 4/4\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.07427\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 068 | loss: 0.07427 -- iter: 4/4\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.06624\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 069 | loss: 0.06624 -- iter: 4/4\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.10904\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 070 | loss: 0.10904 -- iter: 4/4\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.09816\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 071 | loss: 0.09816 -- iter: 4/4\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.08839\u001b[0m\u001b[0m | time: 0.014s\n",
      "| SGD | epoch: 072 | loss: 0.08839 -- iter: 4/4\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.07927\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 073 | loss: 0.07927 -- iter: 4/4\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.07108\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 074 | loss: 0.07108 -- iter: 4/4\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.06380\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 075 | loss: 0.06380 -- iter: 4/4\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.05735\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 076 | loss: 0.05735 -- iter: 4/4\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.05163\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 077 | loss: 0.05163 -- iter: 4/4\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.04654\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 078 | loss: 0.04654 -- iter: 4/4\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.04201\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 079 | loss: 0.04201 -- iter: 4/4\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.03798\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 080 | loss: 0.03798 -- iter: 4/4\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.03438\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 081 | loss: 0.03438 -- iter: 4/4\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.03117\u001b[0m\u001b[0m | time: 0.009s\n",
      "| SGD | epoch: 082 | loss: 0.03117 -- iter: 4/4\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.02826\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 083 | loss: 0.02826 -- iter: 4/4\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.02563\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 084 | loss: 0.02563 -- iter: 4/4\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 085 | loss: 0.02325 -- iter: 4/4\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 086 | loss: 0.02110 -- iter: 4/4\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.01916\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 087 | loss: 0.01916 -- iter: 4/4\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.06289\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 088 | loss: 0.06289 -- iter: 4/4\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.05837\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 089 | loss: 0.05837 -- iter: 4/4\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.05364\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 090 | loss: 0.05364 -- iter: 4/4\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.04848\u001b[0m\u001b[0m | time: 0.011s\n",
      "| SGD | epoch: 091 | loss: 0.04848 -- iter: 4/4\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.04382\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 092 | loss: 0.04382 -- iter: 4/4\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.03961\u001b[0m\u001b[0m | time: 0.016s\n",
      "| SGD | epoch: 093 | loss: 0.03961 -- iter: 4/4\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.03582\u001b[0m\u001b[0m | time: 0.017s\n",
      "| SGD | epoch: 094 | loss: 0.03582 -- iter: 4/4\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.03239\u001b[0m\u001b[0m | time: 0.015s\n",
      "| SGD | epoch: 095 | loss: 0.03239 -- iter: 4/4\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.02930\u001b[0m\u001b[0m | time: 0.013s\n",
      "| SGD | epoch: 096 | loss: 0.02930 -- iter: 4/4\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.02651\u001b[0m\u001b[0m | time: 0.018s\n",
      "| SGD | epoch: 097 | loss: 0.02651 -- iter: 4/4\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 098 | loss: 0.02400 -- iter: 4/4\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.02173\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 099 | loss: 0.02173 -- iter: 4/4\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.01968\u001b[0m\u001b[0m | time: 0.012s\n",
      "| SGD | epoch: 100 | loss: 0.01968 -- iter: 4/4\n",
      "--\n",
      "Testing AND operator\n",
      "0 and 0: [[6.585686e-05]]\n",
      "0 and 1: [[0.03445943]]\n",
      "1 and 0: [[0.03980374]]\n",
      "1 and 1: [[0.95738006]]\n"
     ]
    }
   ],
   "source": [
    "# Logical AND operator\n",
    "X = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\n",
    "Y = [[0.], [0.], [0.], [1.]]\n",
    "\n",
    "# Graph definition\n",
    "with tf.Graph().as_default():\n",
    "    g = tflearn.input_data(shape=[None, 2])\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 128, activation='linear')\n",
    "    g = tflearn.fully_connected(g, 1, activation='sigmoid')\n",
    "    g = tflearn.regression(g, optimizer='sgd', learning_rate=2.,\n",
    "                           loss='mean_square')\n",
    "\n",
    "    # Model training\n",
    "    m = tflearn.DNN(g)\n",
    "    m.fit(X, Y, n_epoch=100, snapshot_epoch=True)\n",
    "\n",
    "    # Test model\n",
    "    print(\"Testing AND operator\")\n",
    "    print(\"0 and 0:\", m.predict([[0., 0.]]))\n",
    "    print(\"0 and 1:\", m.predict([[0., 1.]]))\n",
    "    print(\"1 and 0:\", m.predict([[1., 0.]]))\n",
    "    print(\"1 and 1:\", m.predict([[1., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 5.900e+01 2.540e+01 2.692e+00 3.300e-01 8.220e+00 3.443e+01\n",
      "  5.800e-01 2.685e+00 2.490e+01 9.700e+00 3.425e+03 1.898e+03 2.800e-01\n",
      "  5.900e+00 2.200e+01]\n",
      " [1.000e+00 1.266e+02 2.510e+01 2.649e+00 3.800e-01 6.750e+00 4.978e+01\n",
      "  6.700e-01 2.710e+00 4.210e+01 1.640e+01 4.536e+03 2.486e+03 2.900e-01\n",
      "  1.266e+01 3.500e+01]\n",
      " [2.000e+00 1.245e+02 2.780e+01 2.691e+00 3.400e-01 7.650e+00 7.823e+01\n",
      "  6.700e-01 2.863e+00 2.500e+01 1.030e+01 3.251e+03 1.969e+03 2.100e-01\n",
      "  1.245e+01 5.900e+01]\n",
      " [0.000e+00 9.170e+01 3.120e+01 2.641e+00 3.300e-01 5.600e+00 8.154e+01\n",
      "  7.000e-01 2.734e+00 1.670e+01 7.000e+00 2.641e+03 1.637e+03 1.900e-01\n",
      "  9.170e+00 6.500e+01]\n",
      " [0.000e+00 1.744e+02 2.020e+01 2.661e+00 3.200e-01 8.510e+00 5.541e+01\n",
      "  6.700e-01 2.758e+00 4.770e+01 1.810e+01 5.045e+03 2.618e+03 3.200e-01\n",
      "  1.744e+01 3.800e+01]\n",
      " [3.000e+00 7.420e+01 2.610e+01 2.663e+00 2.500e-01 5.430e+00 6.137e+01\n",
      "  7.000e-01 2.831e+00 3.780e+01 1.510e+01 4.146e+03 2.379e+03 2.500e-01\n",
      "  7.420e+00 4.500e+01]\n",
      " [2.000e+00 9.610e+01 2.840e+01 2.720e+00 1.000e-01 4.970e+00 6.852e+01\n",
      "  6.300e-01 2.629e+00 3.550e+01 1.450e+01 3.866e+03 2.316e+03 2.200e-01\n",
      "  9.610e+00 5.200e+01]\n",
      " [3.000e+00 1.223e+02 2.200e+01 2.631e+00 3.000e-01 6.700e+00 6.371e+01\n",
      "  6.000e-01 2.661e+00 4.110e+01 1.570e+01 4.626e+03 2.442e+03 3.100e-01\n",
      "  1.223e+01 4.400e+01]\n",
      " [2.000e+00 3.490e+01 2.940e+01 2.617e+00 4.400e-01 1.004e+01 6.412e+01\n",
      "  6.200e-01 2.694e+00 3.740e+01 1.490e+01 4.134e+03 2.371e+03 2.500e-01\n",
      "  3.490e+00 4.700e+01]]\n",
      "[4. 2. 2. 2. 3. 2. 2. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"DataNN_formatted.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:16]\n",
    "Y = dataset[:,16]\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "\n",
    "# Download the Titanic dataset\n",
    "# from tflearn.datasets import titanic\n",
    "# titanic.download_dataset('titanic_dataset.csv')\n",
    "\n",
    "# Load CSV file, indicate that the first column represents labels\n",
    "from tflearn.data_utils import load_csv\n",
    "data, labels = load_csv('DataNN_formatted.csv', target_column=0, categorical_labels=True, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, 19])\n",
    "net = tflearn.fully_connected(net, 32)\n",
    "net = tflearn.fully_connected(net, 32)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-e08edf15c512>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Start training (apply gradient descent algorithm)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# model.fit(data, labels, n_epoch=100, snapshot_epoch=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[1;32m--> 184\u001b[1;33m                                       self.targets)\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tflearn\\utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[1;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;31m# If a dict is provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = tflearn.DNN(net)\n",
    "# Start training (apply gradient descent algorithm)\n",
    "# model.fit(data, labels, n_epoch=100, snapshot_epoch=True)\n",
    "model.fit(data, labels, n_epoch=10, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
